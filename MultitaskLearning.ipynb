{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultitaskLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "636deaddf7da4be796cad83bdd0fb1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dc91a746555f4f4aa5a6cb5f97541ebe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_53942d53130b46878e5fab21eb35d0b1",
              "IPY_MODEL_6453645e5a1d42da9040de4865c7b226",
              "IPY_MODEL_6314de54a1f54a0d923e26e41e1dcd63"
            ]
          }
        },
        "dc91a746555f4f4aa5a6cb5f97541ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53942d53130b46878e5fab21eb35d0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3729fdc46bc4462ca163eec4502646bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c31cc0b4569c4eb7ab4e1e3f5e497d32"
          }
        },
        "6453645e5a1d42da9040de4865c7b226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bf5dab1dc254cf399af2157a3f3a0a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87319819,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87319819,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f8c1dcaebf1446da462223a703e3f32"
          }
        },
        "6314de54a1f54a0d923e26e41e1dcd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c0d66f0dc9b44c4b77a24f6fc634a87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:03&lt;00:00, 19.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58bcf833d3914c869526f32e63c0a3fa"
          }
        },
        "3729fdc46bc4462ca163eec4502646bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c31cc0b4569c4eb7ab4e1e3f5e497d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bf5dab1dc254cf399af2157a3f3a0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f8c1dcaebf1446da462223a703e3f32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c0d66f0dc9b44c4b77a24f6fc634a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58bcf833d3914c869526f32e63c0a3fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "THK_o2Ud0NFH"
      },
      "outputs": [],
      "source": [
        "# Trying out quick concept of MTL:\n",
        "# Ref: https://towardsdatascience.com/multi-task-learning-with-pytorch-and-fastai-6d10dc7ce855\n",
        "#     : https://ruder.io/multi-task/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Task Learning (MTL) model is a model that is able to do more than one task. It is as simple as that. \n",
        "# In general, as soon as you find yourself optimizing more than one loss function, you are effectively doing MTL."
      ],
      "metadata": {
        "id": "0zGV2alX4bV2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: https://www.kaggle.com/jangedoo/utkface-new\n",
        "# Datatype: Image - Labels: Age,Gender,Ethnicity"
      ],
      "metadata": {
        "id": "yMMbctln5Ae6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# “MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks”."
      ],
      "metadata": {
        "id": "czAoHGf75O0J"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The focus on running this experiment is not more on finetuning/data augmentation but more of trying gain a practical aspect of multitask learning, trying to optimize multiple losses at a time. "
      ],
      "metadata": {
        "id": "QEtvn9fP6RGe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the \"tarfile\" module\n",
        "import tarfile"
      ],
      "metadata": {
        "id": "hyU_wGzDDcWi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"UTKFace.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('./UTKFaceFolder')"
      ],
      "metadata": {
        "id": "bXvsIkkOhKvi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(\"crop_part1.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('./CropPart1')"
      ],
      "metadata": {
        "id": "_uGthHFJtK0Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open file\n",
        "\"\"\"utkfacefile = tarfile.open('UTKFace.tar.gz')\n",
        "# extracting file\n",
        "utkfacefile.extractall('./UTKFaceFolder')\n",
        "utkfacefile.close()\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2ftsV7S_DtkC",
        "outputId": "7577e205-4a9c-449d-bba8-86454415ce27"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"utkfacefile = tarfile.open('UTKFace.tar.gz')\\n# extracting file\\nutkfacefile.extractall('./UTKFaceFolder')\\nutkfacefile.close()\""
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# open file\n",
        "crop_part1 = tarfile.open('crop_part1.tar.gz')\n",
        "# extracting file\n",
        "crop_part1.extractall('./CropPart1')\n",
        "crop_part1.close()\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K1lQE8TPHOup",
        "outputId": "b11db567-edf3-4651-8d9e-ecfb0d421da1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# open file\\ncrop_part1 = tarfile.open('crop_part1.tar.gz')\\n# extracting file\\ncrop_part1.extractall('./CropPart1')\\ncrop_part1.close()\""
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.layers import MSELossFlat, CrossEntropyFlat\n",
        "from torchvision import transforms\n",
        "import warnings"
      ],
      "metadata": {
        "id": "SnFRGS7lQADe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image files in path recursively, only in folders, if specified.\n",
        "# This is simply get_files called with a list of standard image extensions."
      ],
      "metadata": {
        "id": "Zup2zyaYO7Up"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_train = get_image_files(\"UTKFaceFolder/UTKFace\")\n",
        "files_validation = get_image_files(\"CropPart1/crop_part1\")"
      ],
      "metadata": {
        "id": "ytHzyt0yNBB1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(files_train), len(files_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNUjI51SQcqk",
        "outputId": "aac70f67-6e62-4370-8388-a249a985b416"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23708, 9779)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels of each face image is embedded in the file name, formated like [age]_[gender]_[race]_[date&time].jpg\n",
        "\n",
        "[age] is an integer from 0 to 116, indicating the age\n",
        "[gender] is either 0 (male) or 1 (female)\n",
        "[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).\n",
        "[date&time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace"
      ],
      "metadata": {
        "id": "R7F3E_bQS780"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame(files_train,columns=[\"name\"])\n",
        "df_train.name = df_train.name.apply(str)"
      ],
      "metadata": {
        "id": "BHLNhMNcQzEj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "hdHFMaqQSVhc",
        "outputId": "e698ade2-1ff0-49ff-c1ff-23b099b24745"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d7e4502a-cb78-44fb-a232-74e842328063\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UTKFaceFolder/UTKFace/46_1_0_20170110151342799...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UTKFaceFolder/UTKFace/8_1_0_20170109202314510....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UTKFaceFolder/UTKFace/26_0_2_20170116181354956...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>UTKFaceFolder/UTKFace/24_0_1_20170113151024815...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UTKFaceFolder/UTKFace/27_0_1_20170117164420634...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23703</th>\n",
              "      <td>UTKFaceFolder/UTKFace/35_1_0_20170117185628434...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23704</th>\n",
              "      <td>UTKFaceFolder/UTKFace/28_0_1_20170117020909688...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23705</th>\n",
              "      <td>UTKFaceFolder/UTKFace/30_0_0_20170117181232946...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23706</th>\n",
              "      <td>UTKFaceFolder/UTKFace/28_0_1_20170116192030399...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23707</th>\n",
              "      <td>UTKFaceFolder/UTKFace/3_1_2_20161219151926867....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23708 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7e4502a-cb78-44fb-a232-74e842328063')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7e4502a-cb78-44fb-a232-74e842328063 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7e4502a-cb78-44fb-a232-74e842328063');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    name\n",
              "0      UTKFaceFolder/UTKFace/46_1_0_20170110151342799...\n",
              "1      UTKFaceFolder/UTKFace/8_1_0_20170109202314510....\n",
              "2      UTKFaceFolder/UTKFace/26_0_2_20170116181354956...\n",
              "3      UTKFaceFolder/UTKFace/24_0_1_20170113151024815...\n",
              "4      UTKFaceFolder/UTKFace/27_0_1_20170117164420634...\n",
              "...                                                  ...\n",
              "23703  UTKFaceFolder/UTKFace/35_1_0_20170117185628434...\n",
              "23704  UTKFaceFolder/UTKFace/28_0_1_20170117020909688...\n",
              "23705  UTKFaceFolder/UTKFace/30_0_0_20170117181232946...\n",
              "23706  UTKFaceFolder/UTKFace/28_0_1_20170116192030399...\n",
              "23707  UTKFaceFolder/UTKFace/3_1_2_20161219151926867....\n",
              "\n",
              "[23708 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get label\n",
        "df_train[\"label\"] = df_train.name.apply(lambda x: re.findall(r\"\\d{1,3}_\\d_\\d\",x)[0])"
      ],
      "metadata": {
        "id": "w4ROgOGdWRnv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"label\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfQM7-gTWT0u",
        "outputId": "a67f26f2-67df-490c-946c-b56aa300051d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        46_1_0\n",
              "1         8_1_0\n",
              "2        26_0_2\n",
              "3        24_0_1\n",
              "4        27_0_1\n",
              "          ...  \n",
              "23703    35_1_0\n",
              "23704    28_0_1\n",
              "23705    30_0_0\n",
              "23706    28_0_1\n",
              "23707     3_1_2\n",
              "Name: label, Length: 23708, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace _ with \" \"\n",
        "df_train[\"label\"] = df_train.label.apply(lambda x: re.sub(\"_\",\" \", x))\n",
        "df_train[\"label\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tlUZZJEW3VW",
        "outputId": "39e3cff7-5e0e-4c83-e9d3-63f67feabc22"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        46 1 0\n",
              "1         8 1 0\n",
              "2        26 0 2\n",
              "3        24 0 1\n",
              "4        27 0 1\n",
              "          ...  \n",
              "23703    35 1 0\n",
              "23704    28 0 1\n",
              "23705    30 0 0\n",
              "23706    28 0 1\n",
              "23707     3 1 2\n",
              "Name: label, Length: 23708, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"age\"] = df_train.label.apply(lambda x: int(x.split(\" \")[0]))\n",
        "df_train[\"gender\"] = df_train.label.apply(lambda x: int(x.split(\" \")[1]))\n",
        "df_train[\"ethnicity\"] = df_train.label.apply(lambda x: int(x.split(\" \")[2]))"
      ],
      "metadata": {
        "id": "sv5N5vIJYuc1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yzvedYTCaBHj",
        "outputId": "094a6e3d-1995-442e-cd0a-d2e1158fb8cc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-df5855d3-4825-4f86-b962-77f510cf66db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>label</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>ethnicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UTKFaceFolder/UTKFace/46_1_0_20170110151342799...</td>\n",
              "      <td>46 1 0</td>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UTKFaceFolder/UTKFace/8_1_0_20170109202314510....</td>\n",
              "      <td>8 1 0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UTKFaceFolder/UTKFace/26_0_2_20170116181354956...</td>\n",
              "      <td>26 0 2</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>UTKFaceFolder/UTKFace/24_0_1_20170113151024815...</td>\n",
              "      <td>24 0 1</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UTKFaceFolder/UTKFace/27_0_1_20170117164420634...</td>\n",
              "      <td>27 0 1</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df5855d3-4825-4f86-b962-77f510cf66db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df5855d3-4825-4f86-b962-77f510cf66db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df5855d3-4825-4f86-b962-77f510cf66db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                name   label  ...  gender  ethnicity\n",
              "0  UTKFaceFolder/UTKFace/46_1_0_20170110151342799...  46 1 0  ...       1          0\n",
              "1  UTKFaceFolder/UTKFace/8_1_0_20170109202314510....   8 1 0  ...       1          0\n",
              "2  UTKFaceFolder/UTKFace/26_0_2_20170116181354956...  26 0 2  ...       0          2\n",
              "3  UTKFaceFolder/UTKFace/24_0_1_20170113151024815...  24 0 1  ...       0          1\n",
              "4  UTKFaceFolder/UTKFace/27_0_1_20170117164420634...  27 0 1  ...       0          1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation = pd.DataFrame(files_validation,columns=[\"name\"])\n",
        "df_validation.name = df_validation.name.apply(str)\n",
        "df_validation[\"label\"] = df_validation.name.apply(lambda x: re.findall(r\"\\d{1,3}_\\d_\\d\",x)[0])\n",
        "df_validation[\"label\"] = df_validation.label.apply(lambda x: re.sub(\"_\",\" \", x))\n",
        "df_validation[\"age\"] = df_validation.label.apply(lambda x: int(x.split(\" \")[0]))\n",
        "df_validation[\"gender\"] = df_validation.label.apply(lambda x: int(x.split(\" \")[1]))\n",
        "df_validation[\"ethnicity\"] = df_validation.label.apply(lambda x: int(x.split(\" \")[2]))"
      ],
      "metadata": {
        "id": "iwr_SkSXbcVM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T-mGs1jtbfBT",
        "outputId": "e4c679b1-b9c6-4c26-c756-152fa4900a83"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7572b383-1ff3-4b8b-a558-fa620090fcc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>label</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>ethnicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CropPart1/crop_part1/46_1_0_20170110151342799....</td>\n",
              "      <td>46 1 0</td>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CropPart1/crop_part1/8_1_0_20170109202314510.j...</td>\n",
              "      <td>8 1 0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CropPart1/crop_part1/1_0_0_20161219191012803.j...</td>\n",
              "      <td>1 0 0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CropPart1/crop_part1/47_0_0_20170104211911013....</td>\n",
              "      <td>47 0 0</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CropPart1/crop_part1/2_0_3_20161219230614992.j...</td>\n",
              "      <td>2 0 3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7572b383-1ff3-4b8b-a558-fa620090fcc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7572b383-1ff3-4b8b-a558-fa620090fcc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7572b383-1ff3-4b8b-a558-fa620090fcc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                name   label  ...  gender  ethnicity\n",
              "0  CropPart1/crop_part1/46_1_0_20170110151342799....  46 1 0  ...       1          0\n",
              "1  CropPart1/crop_part1/8_1_0_20170109202314510.j...   8 1 0  ...       1          0\n",
              "2  CropPart1/crop_part1/1_0_0_20161219191012803.j...   1 0 0  ...       0          0\n",
              "3  CropPart1/crop_part1/47_0_0_20170104211911013....  47 0 0  ...       0          0\n",
              "4  CropPart1/crop_part1/2_0_3_20161219230614992.j...   2 0 3  ...       0          3\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking distribution for ages\n",
        "df_train.age.hist()\n",
        "df_validation.age.hist()\n",
        "plt.legend([\"train\",\"valid\"]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "USpWhvPXdzKu",
        "outputId": "abb11370-02cb-4948-d01b-5a2b0a7d2b55"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#should be only 0s and 1s\n",
        "df_train.gender.unique(), df_validation.gender.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfVUc9DffDK-",
        "outputId": "f3224d97-1c17-4b8e-ca0d-c19b10b1062c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 0]), array([1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.ethnicity.unique(), df_validation.ethnicity.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swg66hGhQIsH",
        "outputId": "92df709f-0827-4341-b1ff-81d2f214a232"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 2, 1, 3, 4]), array([0, 3, 1, 4, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fixing the label\n",
        "idx = df_validation[df_validation.gender == 3].index\n",
        "df_validation.loc[idx,\"gender\"] = 1 #1 means woman"
      ],
      "metadata": {
        "id": "NLL1kCX5fOvM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "5oC59p1gGR3G"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self,df, tfms, size=64):\n",
        "        self.paths = list(df.name)\n",
        "        self.labels = list(df.label)\n",
        "        self.tfms = tfms\n",
        "        self.size = size\n",
        "        self.norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #imagenet stats\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        #dealing with the image\n",
        "        img = PIL.Image.open(self.paths[idx]).convert('RGB')\n",
        "        img = Image(pil2tensor(img, dtype=np.float32).div_(255))\n",
        "        img = img.apply_tfms(self.tfms, size = self.size)\n",
        "        img = self.norm(img.data)\n",
        "\n",
        "        #dealing with the labels\n",
        "        labels = self.labels[idx].split(\" \")\n",
        "        age = torch.tensor(float(labels[0]), dtype=torch.float32)\n",
        "        gender = torch.tensor(int(labels[1]), dtype=torch.int64)\n",
        "        ethnicity = torch.tensor(int(labels[2]), dtype=torch.int64)\n",
        "        \n",
        "        return img.data, (age.log_()/4.75, gender, ethnicity)\n",
        "\n",
        "    def show(self,idx):\n",
        "        x,y = self.__getitem__(idx)\n",
        "        age,gender,ethnicity = y\n",
        "        stds = np.array([0.229, 0.224, 0.225])\n",
        "        means = np.array([0.485, 0.456, 0.406])\n",
        "        img = ((x.numpy().transpose((1,2,0))*stds + means)*255).astype(np.uint8)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"{} {} {}\".format(int(age.mul_(4.75).exp_().item()), gender.item(), ethnicity.item()))"
      ],
      "metadata": {
        "id": "WjlFefxzh-Li"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfms = get_transforms()\n",
        "train_ds = MultiTaskDataset(df_train, tfms[0], size=64)\n",
        "valid_ds = MultiTaskDataset(df_validation, tfms[1], size=64)\n",
        "train_dl = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=2)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=512, shuffle=True, num_workers=2)\n",
        "data = DataBunch(train_dl, valid_dl)"
      ],
      "metadata": {
        "id": "xCUlUg-YCiEN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "SlOatNaSGfPP",
        "outputId": "724b464b-fb5e-46e0-9ecb-f39a3015a42a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ds.show(9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "HlNI0jVNGkGc",
        "outputId": "bde2485c-2320-4dec-cded-7b088e61887f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Model:\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, arch,ps=0.5):\n",
        "        super(MultiTaskModel,self).__init__()\n",
        "        self.encoder = create_body(arch)\n",
        "        self.fc1 = create_head(1024,1,ps=ps)\n",
        "        self.fc2 = create_head(1024,2,ps=ps)\n",
        "        self.fc3 = create_head(1024,5,ps=ps)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        age = torch.sigmoid(self.fc1(x))\n",
        "        gender = self.fc2(x)\n",
        "        ethnicity = self.fc3(x)\n",
        "\n",
        "        return [age, gender, ethnicity]"
      ],
      "metadata": {
        "id": "oEFyYEM37WLk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Loss\n",
        "class MultiTaskLossWrapper(nn.Module):\n",
        "    def __init__(self, task_num):\n",
        "        super(MultiTaskLossWrapper, self).__init__()\n",
        "        self.task_num = task_num\n",
        "        self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "\n",
        "    def forward(self, preds, age, gender, ethnicity):\n",
        "\n",
        "        mse, crossEntropy = MSELossFlat(), CrossEntropyFlat()\n",
        "        \n",
        "        sages = (age*4.75).exp_()\n",
        "        idx1 = (sages <20) | ((sages > 40) & (sages <= 60))\n",
        "        idx2 = sages > 60\n",
        "        loss0 = mse(preds[0], age) + 2*mse(preds[0][idx1],age[idx1]) + 3*mse(preds[0][idx2],age[idx2]) #trying to account for the imbalance\n",
        "        print(\"loss0\")\n",
        "        print(loss0)\n",
        "        loss1 = crossEntropy(preds[1],gender)\n",
        "        print(\"loss1\")\n",
        "        print(loss1)\n",
        "        loss2 = crossEntropy(preds[2],ethnicity)\n",
        "        print(\"loss2\")\n",
        "        print(loss2)\n",
        "\n",
        "        precision0 = torch.exp(-self.log_vars[0])\n",
        "        loss0 = precision0*loss0 + self.log_vars[0]\n",
        "\n",
        "        precision1 = torch.exp(-self.log_vars[1])\n",
        "        loss1 = precision1*loss1 + self.log_vars[1]\n",
        "\n",
        "        precision2 = torch.exp(-self.log_vars[2])\n",
        "        loss2 = precision2*loss2 + self.log_vars[2]\n",
        "        \n",
        "        return loss0+loss1+loss2"
      ],
      "metadata": {
        "id": "uGbiB1w2XpLC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse_age(preds, age, gender, ethnicity): return root_mean_squared_error(preds[0],age)\n",
        "def acc_gender(preds, age, gender, ethnicity): return accuracy(preds[1], gender)\n",
        "def acc_ethnicity(preds, age, gender, ethnicity): return accuracy(preds[2], ethnicity)\n",
        "metrics = [rmse_age, acc_gender, acc_ethnicity]"
      ],
      "metadata": {
        "id": "nJ8Ef193Xv95"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskModel(models.resnet34, ps=0.25)\n",
        "\n",
        "loss_func = MultiTaskLossWrapper(3).to(data.device) #just making sure the loss is on the gpu\n",
        "\n",
        "learn = Learner(data, model, loss_func=loss_func, callback_fns=ShowGraph, metrics=metrics)\n",
        "\n",
        "#spliting the model so that I can use discriminative learning rates\n",
        "learn.split([learn.model.encoder[:6],\n",
        "             learn.model.encoder[6:],\n",
        "             nn.ModuleList([learn.model.fc1, learn.model.fc2, learn.model.fc3])]);\n",
        "\n",
        "#first I'll train only the last layer group (the heads)\n",
        "learn.freeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "636deaddf7da4be796cad83bdd0fb1d0",
            "dc91a746555f4f4aa5a6cb5f97541ebe",
            "53942d53130b46878e5fab21eb35d0b1",
            "6453645e5a1d42da9040de4865c7b226",
            "6314de54a1f54a0d923e26e41e1dcd63",
            "3729fdc46bc4462ca163eec4502646bb",
            "c31cc0b4569c4eb7ab4e1e3f5e497d32",
            "6bf5dab1dc254cf399af2157a3f3a0a8",
            "8f8c1dcaebf1446da462223a703e3f32",
            "5c0d66f0dc9b44c4b77a24f6fc634a87",
            "58bcf833d3914c869526f32e63c0a3fa"
          ]
        },
        "id": "-r5iyhcIZp0S",
        "outputId": "897d3084-4e6e-489a-806a-607285716028"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "636deaddf7da4be796cad83bdd0fb1d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zfaiQg_pa9ii",
        "outputId": "a6f437e3-4ecb-430e-e96d-38354012aec6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      33.33% [1/3 13:18<26:36]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>rmse_age</th>\n",
              "      <th>acc_gender</th>\n",
              "      <th>acc_ethnicity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.129963</td>\n",
              "      <td>#na#</td>\n",
              "      <td>13:18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='41' class='' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      87.23% [41/47 11:47<01:43 9.0464]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.8936, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7988, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7860, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8727, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7603, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7504, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.9894, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.8238, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7824, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8496, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7885, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8117, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8876, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7894, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8080, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.9159, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7195, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8108, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7968, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.8045, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8021, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.9064, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7678, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7831, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8545, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7780, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7291, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8009, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7616, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8217, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8719, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7656, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7894, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8532, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7435, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7756, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8433, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7567, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7993, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8387, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7733, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7927, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8792, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7800, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8266, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7773, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7653, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8378, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7993, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7808, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8009, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8564, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7834, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7748, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8066, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7676, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7933, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8369, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7606, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7988, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8451, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7984, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7745, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7856, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7652, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8545, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8628, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7292, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8150, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7797, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7470, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7700, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7978, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7686, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7665, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8349, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7774, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7516, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7740, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7610, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7708, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8431, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7388, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7801, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8238, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7318, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8060, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7974, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7259, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7615, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8795, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7532, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7396, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7056, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7611, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7716, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6801, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7259, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7357, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6542, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7112, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7479, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6261, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7156, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6896, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5726, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6719, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7393, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6125, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6870, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6582, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6206, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7044, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6548, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5645, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6659, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6482, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5419, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6725, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6268, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6075, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6202, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5892, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4684, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6483, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6494, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4707, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6539, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4773, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4774, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6137, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5401, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4018, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6304, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5393, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3655, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5937, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4832, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3720, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5436, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4204, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.3033, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5734, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3985, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3587, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5793, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3824, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3840, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6519, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3660, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3610, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6828, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3292, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3564, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6222, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3830, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3489, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6718, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4076, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3843, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5712, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3545, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3734, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5923, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3489, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4703, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6485, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5493, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3297, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6133, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4814, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4940, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5830, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3774, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3017, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5640, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2521, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3280, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5260, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3283, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2735, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5792, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2700, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3154, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5413, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3299, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2731, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5819, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2427, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1917, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5776, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2110, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2662, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4979, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1729, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2013, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4945, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1582, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1684, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5360, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1570, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2237, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6524, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2826, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1277, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7431, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1939, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1541, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5334, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2349, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1319, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7163, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1367, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1465, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(1.3053, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2793, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1306, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6984, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3051, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1983, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(1.5001, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2494, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1451, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6777, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5817, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1873, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.8764, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4308, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1568, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5094, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8514, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1552, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6029, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.9676, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2136, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.8100, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4702, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2014, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(1.3934, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4275, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3747, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.8164, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.9065, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2399, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(2.4721, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(4.2586, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3714, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(3.0192, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(2.6114, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4286, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(6.1964, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(4.1528, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5662, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(1.5955, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(16.3695, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3755, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(24.7847, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(16.6108, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(1.3163, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(8.7923, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(21.0161, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7834, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(39.2452, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(133.5024, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.9840, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(46.2869, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(49.8351, grad_fn=<NllLossBackward0>)\n",
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(10,max_lr=slice(1e-6,3e-4),\n",
        "                   callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='trial')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H9EWJeo3cJaE",
        "outputId": "770da745-0aea-49bc-9014-d74693a50896"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>rmse_age</th>\n",
              "      <th>acc_gender</th>\n",
              "      <th>acc_ethnicity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.921134</td>\n",
              "      <td>2.373713</td>\n",
              "      <td>0.268861</td>\n",
              "      <td>0.694140</td>\n",
              "      <td>0.447387</td>\n",
              "      <td>15:55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.472892</td>\n",
              "      <td>1.973755</td>\n",
              "      <td>0.237043</td>\n",
              "      <td>0.722467</td>\n",
              "      <td>0.555681</td>\n",
              "      <td>15:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.230764</td>\n",
              "      <td>1.832078</td>\n",
              "      <td>0.222408</td>\n",
              "      <td>0.738726</td>\n",
              "      <td>0.588915</td>\n",
              "      <td>15:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.087118</td>\n",
              "      <td>1.744768</td>\n",
              "      <td>0.205771</td>\n",
              "      <td>0.747622</td>\n",
              "      <td>0.604970</td>\n",
              "      <td>15:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.978931</td>\n",
              "      <td>1.675935</td>\n",
              "      <td>0.193163</td>\n",
              "      <td>0.749668</td>\n",
              "      <td>0.623990</td>\n",
              "      <td>15:56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.900453</td>\n",
              "      <td>1.640144</td>\n",
              "      <td>0.192787</td>\n",
              "      <td>0.750793</td>\n",
              "      <td>0.634830</td>\n",
              "      <td>16:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.834824</td>\n",
              "      <td>1.599597</td>\n",
              "      <td>0.186498</td>\n",
              "      <td>0.756008</td>\n",
              "      <td>0.639534</td>\n",
              "      <td>16:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.797749</td>\n",
              "      <td>1.581937</td>\n",
              "      <td>0.179420</td>\n",
              "      <td>0.759689</td>\n",
              "      <td>0.638818</td>\n",
              "      <td>16:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.772240</td>\n",
              "      <td>1.580734</td>\n",
              "      <td>0.179609</td>\n",
              "      <td>0.758667</td>\n",
              "      <td>0.638000</td>\n",
              "      <td>16:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.764238</td>\n",
              "      <td>1.570621</td>\n",
              "      <td>0.179266</td>\n",
              "      <td>0.758462</td>\n",
              "      <td>0.642397</td>\n",
              "      <td>16:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.9004, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7863, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7899, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8773, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7810, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8108, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7588, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7902, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8103, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8825, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7793, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7875, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8146, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7232, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8186, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7974, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7671, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8039, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7220, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7594, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7604, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7556, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7614, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.8131, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7373, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7836, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7695, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8421, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7411, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7780, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.8163, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7301, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7289, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7457, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7196, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7505, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6742, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7294, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7187, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7814, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7088, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6868, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6844, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7149, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7828, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6360, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7517, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7696, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6972, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6937, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6979, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.7102, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7299, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7890, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6569, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6966, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6936, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6679, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6727, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7220, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6484, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6921, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7138, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6803, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.7063, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7291, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6454, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6838, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7189, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6366, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6794, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7168, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5836, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6527, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.7008, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.6241, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6494, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6506, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5483, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6510, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6243, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5582, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6742, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6701, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5878, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6400, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6343, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5365, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6798, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6372, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4819, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6102, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6705, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5154, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6640, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6489, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4760, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6737, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6199, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5644, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6398, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6273, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4867, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6544, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6282, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5999, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5986, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5931, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4984, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6541, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5411, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4885, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6111, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.6072, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5070, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5865, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5359, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4436, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6307, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5414, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4510, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6357, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5657, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.5085, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6103, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5439, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4673, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6118, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5570, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4361, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6207, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5769, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4842, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5872, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4983, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4671, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6200, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5295, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4865, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6833, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5096, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.3979)\n",
            "loss1\n",
            "tensor(0.5865)\n",
            "loss2\n",
            "tensor(1.3973)\n",
            "loss0\n",
            "tensor(0.4189)\n",
            "loss1\n",
            "tensor(0.5662)\n",
            "loss2\n",
            "tensor(1.4512)\n",
            "loss0\n",
            "tensor(0.4038)\n",
            "loss1\n",
            "tensor(0.5732)\n",
            "loss2\n",
            "tensor(1.3746)\n",
            "loss0\n",
            "tensor(0.3309)\n",
            "loss1\n",
            "tensor(0.5653)\n",
            "loss2\n",
            "tensor(1.4067)\n",
            "loss0\n",
            "tensor(0.3811)\n",
            "loss1\n",
            "tensor(0.5788)\n",
            "loss2\n",
            "tensor(1.3873)\n",
            "loss0\n",
            "tensor(0.4131)\n",
            "loss1\n",
            "tensor(0.5370)\n",
            "loss2\n",
            "tensor(1.3851)\n",
            "loss0\n",
            "tensor(0.3852)\n",
            "loss1\n",
            "tensor(0.5662)\n",
            "loss2\n",
            "tensor(1.3987)\n",
            "loss0\n",
            "tensor(0.3288)\n",
            "loss1\n",
            "tensor(0.6202)\n",
            "loss2\n",
            "tensor(1.3488)\n",
            "loss0\n",
            "tensor(0.3921)\n",
            "loss1\n",
            "tensor(0.5574)\n",
            "loss2\n",
            "tensor(1.3759)\n",
            "loss0\n",
            "tensor(0.4308)\n",
            "loss1\n",
            "tensor(0.5905)\n",
            "loss2\n",
            "tensor(1.4163)\n",
            "loss0\n",
            "tensor(0.3170)\n",
            "loss1\n",
            "tensor(0.6209)\n",
            "loss2\n",
            "tensor(1.4130)\n",
            "loss0\n",
            "tensor(0.3823)\n",
            "loss1\n",
            "tensor(0.5831)\n",
            "loss2\n",
            "tensor(1.3513)\n",
            "loss0\n",
            "tensor(0.4159)\n",
            "loss1\n",
            "tensor(0.5930)\n",
            "loss2\n",
            "tensor(1.4060)\n",
            "loss0\n",
            "tensor(0.3928)\n",
            "loss1\n",
            "tensor(0.6078)\n",
            "loss2\n",
            "tensor(1.4299)\n",
            "loss0\n",
            "tensor(0.3840)\n",
            "loss1\n",
            "tensor(0.5699)\n",
            "loss2\n",
            "tensor(1.3893)\n",
            "loss0\n",
            "tensor(0.4555)\n",
            "loss1\n",
            "tensor(0.5628)\n",
            "loss2\n",
            "tensor(1.3899)\n",
            "loss0\n",
            "tensor(0.3889)\n",
            "loss1\n",
            "tensor(0.5916)\n",
            "loss2\n",
            "tensor(1.4476)\n",
            "loss0\n",
            "tensor(0.4139)\n",
            "loss1\n",
            "tensor(0.5801)\n",
            "loss2\n",
            "tensor(1.4101)\n",
            "loss0\n",
            "tensor(0.4549)\n",
            "loss1\n",
            "tensor(0.6059)\n",
            "loss2\n",
            "tensor(1.3896)\n",
            "loss0\n",
            "tensor(0.4324)\n",
            "loss1\n",
            "tensor(0.5260)\n",
            "loss2\n",
            "tensor(1.2897)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Better model found at epoch 0 with valid_loss value: 2.3737125396728516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.4455, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6210, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3574, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6506, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5376, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3637, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6215, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.5294, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4061, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6191, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4527, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4395, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6075, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3848, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3896, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5395, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4500, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4316, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5666, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4356, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4180, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5574, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4327, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4561, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5908, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4591, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3199, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5911, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4468, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4208, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5606, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4703, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3670, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5946, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4207, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3177, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5723, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4879, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3542, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5767, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4385, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4417, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5886, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4465, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3629, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5593, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3774, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4471, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5700, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4146, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4067, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5358, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3516, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3922, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5580, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3969, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3066, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5440, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4288, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4127, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5749, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4125, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3039, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6060, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3791, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3522, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5776, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4135, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3788, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5733, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3979, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3534, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5855, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3794, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3932, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5439, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.4019, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3165, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5216, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2571, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4163, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5412, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3758, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3818, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5766, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2977, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3674, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5696, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3849, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3004, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6128, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3952, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4089, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5880, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3665, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4503, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5483, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3460, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3074, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.6206, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3909, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3483, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5720, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3126, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2993, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5634, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2778, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3447, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5738, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3073, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3553, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5728, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3380, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.4104, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5138, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3381, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2866, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5611, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3099, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3551, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5689, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2686, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3579, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5485, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3270, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3478, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5051, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3189, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2989, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5429, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3352, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3707, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5555, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3147, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3048, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5461, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3267, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2451, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5310, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3206, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2593)\n",
            "loss1\n",
            "tensor(0.5297)\n",
            "loss2\n",
            "tensor(1.1842)\n",
            "loss0\n",
            "tensor(0.2464)\n",
            "loss1\n",
            "tensor(0.5209)\n",
            "loss2\n",
            "tensor(1.1477)\n",
            "loss0\n",
            "tensor(0.2476)\n",
            "loss1\n",
            "tensor(0.5075)\n",
            "loss2\n",
            "tensor(1.1410)\n",
            "loss0\n",
            "tensor(0.2731)\n",
            "loss1\n",
            "tensor(0.5402)\n",
            "loss2\n",
            "tensor(1.2067)\n",
            "loss0\n",
            "tensor(0.2343)\n",
            "loss1\n",
            "tensor(0.5031)\n",
            "loss2\n",
            "tensor(1.1987)\n",
            "loss0\n",
            "tensor(0.2489)\n",
            "loss1\n",
            "tensor(0.5197)\n",
            "loss2\n",
            "tensor(1.1410)\n",
            "loss0\n",
            "tensor(0.2646)\n",
            "loss1\n",
            "tensor(0.5681)\n",
            "loss2\n",
            "tensor(1.1430)\n",
            "loss0\n",
            "tensor(0.3016)\n",
            "loss1\n",
            "tensor(0.5452)\n",
            "loss2\n",
            "tensor(1.1318)\n",
            "loss0\n",
            "tensor(0.2636)\n",
            "loss1\n",
            "tensor(0.5659)\n",
            "loss2\n",
            "tensor(1.2480)\n",
            "loss0\n",
            "tensor(0.3215)\n",
            "loss1\n",
            "tensor(0.5430)\n",
            "loss2\n",
            "tensor(1.2448)\n",
            "loss0\n",
            "tensor(0.2614)\n",
            "loss1\n",
            "tensor(0.5357)\n",
            "loss2\n",
            "tensor(1.1756)\n",
            "loss0\n",
            "tensor(0.2792)\n",
            "loss1\n",
            "tensor(0.4957)\n",
            "loss2\n",
            "tensor(1.1790)\n",
            "loss0\n",
            "tensor(0.2766)\n",
            "loss1\n",
            "tensor(0.5355)\n",
            "loss2\n",
            "tensor(1.1764)\n",
            "loss0\n",
            "tensor(0.2278)\n",
            "loss1\n",
            "tensor(0.5201)\n",
            "loss2\n",
            "tensor(1.1695)\n",
            "loss0\n",
            "tensor(0.2334)\n",
            "loss1\n",
            "tensor(0.5353)\n",
            "loss2\n",
            "tensor(1.1889)\n",
            "loss0\n",
            "tensor(0.2789)\n",
            "loss1\n",
            "tensor(0.5163)\n",
            "loss2\n",
            "tensor(1.1597)\n",
            "loss0\n",
            "tensor(0.2879)\n",
            "loss1\n",
            "tensor(0.5287)\n",
            "loss2\n",
            "tensor(1.1619)\n",
            "loss0\n",
            "tensor(0.2450)\n",
            "loss1\n",
            "tensor(0.5717)\n",
            "loss2\n",
            "tensor(1.1930)\n",
            "loss0\n",
            "tensor(0.2322)\n",
            "loss1\n",
            "tensor(0.5544)\n",
            "loss2\n",
            "tensor(1.1712)\n",
            "loss0\n",
            "tensor(0.3168)\n",
            "loss1\n",
            "tensor(0.7194)\n",
            "loss2\n",
            "tensor(1.1323)\n",
            "Better model found at epoch 1 with valid_loss value: 1.9737550020217896.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2836, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5763, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2933, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3250, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5045, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2884, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3773, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5679, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2416, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2943, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5631, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2891, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2742, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5920, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3643, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3295, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5169, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2926, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3184, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5421, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2880, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2967, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5680, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2871, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2789, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5288, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3263, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2725, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5020, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2128, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3267, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5125, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2838, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3150, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4834, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2745, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3001, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5640, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2735, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3038, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5557, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2718, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3196, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5305, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3076, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3141, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5353, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3001, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3210, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4980, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2705, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2738, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5646, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2697, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3515, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5863, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3136, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3396, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5553, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2783, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2634, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5508, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2439, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2841, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5557, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2724, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3768, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5566, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2996, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3230, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5328, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3128, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2466, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5128, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2974, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3299, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5432, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2841, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3102, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5653, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2862, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2804, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5293, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2784, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3511, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5398, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2769, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3591, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5212, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2608, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2466, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5481, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2310, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3013, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5094, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2259, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3280, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5045, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2827, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2467, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5448, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2365, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2076, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5468, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2121, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2644, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5088, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2496, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2828, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5443, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2313, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3115, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5426, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2205, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2903, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4861, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2920, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3226, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5657, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2730, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2782, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5591, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3330, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2807, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4929, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2417, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2346, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5409, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1814, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3404, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5349, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2838, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2386, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5320, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2794, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2910, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5872, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2194, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1925, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5834, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2227, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2006)\n",
            "loss1\n",
            "tensor(0.5320)\n",
            "loss2\n",
            "tensor(1.1071)\n",
            "loss0\n",
            "tensor(0.2057)\n",
            "loss1\n",
            "tensor(0.5462)\n",
            "loss2\n",
            "tensor(1.0658)\n",
            "loss0\n",
            "tensor(0.1991)\n",
            "loss1\n",
            "tensor(0.5273)\n",
            "loss2\n",
            "tensor(1.0886)\n",
            "loss0\n",
            "tensor(0.2421)\n",
            "loss1\n",
            "tensor(0.5049)\n",
            "loss2\n",
            "tensor(1.1002)\n",
            "loss0\n",
            "tensor(0.2621)\n",
            "loss1\n",
            "tensor(0.4863)\n",
            "loss2\n",
            "tensor(1.1004)\n",
            "loss0\n",
            "tensor(0.1883)\n",
            "loss1\n",
            "tensor(0.5393)\n",
            "loss2\n",
            "tensor(1.0822)\n",
            "loss0\n",
            "tensor(0.2218)\n",
            "loss1\n",
            "tensor(0.5077)\n",
            "loss2\n",
            "tensor(1.1065)\n",
            "loss0\n",
            "tensor(0.2733)\n",
            "loss1\n",
            "tensor(0.4936)\n",
            "loss2\n",
            "tensor(1.0987)\n",
            "loss0\n",
            "tensor(0.2439)\n",
            "loss1\n",
            "tensor(0.5095)\n",
            "loss2\n",
            "tensor(1.0567)\n",
            "loss0\n",
            "tensor(0.1995)\n",
            "loss1\n",
            "tensor(0.4594)\n",
            "loss2\n",
            "tensor(1.1484)\n",
            "loss0\n",
            "tensor(0.1875)\n",
            "loss1\n",
            "tensor(0.4792)\n",
            "loss2\n",
            "tensor(1.1195)\n",
            "loss0\n",
            "tensor(0.2225)\n",
            "loss1\n",
            "tensor(0.5088)\n",
            "loss2\n",
            "tensor(1.0171)\n",
            "loss0\n",
            "tensor(0.2116)\n",
            "loss1\n",
            "tensor(0.4876)\n",
            "loss2\n",
            "tensor(1.1075)\n",
            "loss0\n",
            "tensor(0.2431)\n",
            "loss1\n",
            "tensor(0.5805)\n",
            "loss2\n",
            "tensor(1.1804)\n",
            "loss0\n",
            "tensor(0.2510)\n",
            "loss1\n",
            "tensor(0.5133)\n",
            "loss2\n",
            "tensor(1.0885)\n",
            "loss0\n",
            "tensor(0.1820)\n",
            "loss1\n",
            "tensor(0.5182)\n",
            "loss2\n",
            "tensor(1.0824)\n",
            "loss0\n",
            "tensor(0.2304)\n",
            "loss1\n",
            "tensor(0.5309)\n",
            "loss2\n",
            "tensor(1.0834)\n",
            "loss0\n",
            "tensor(0.2158)\n",
            "loss1\n",
            "tensor(0.5203)\n",
            "loss2\n",
            "tensor(1.1173)\n",
            "loss0\n",
            "tensor(0.2246)\n",
            "loss1\n",
            "tensor(0.5258)\n",
            "loss2\n",
            "tensor(1.0851)\n",
            "loss0\n",
            "tensor(0.2020)\n",
            "loss1\n",
            "tensor(0.4247)\n",
            "loss2\n",
            "tensor(1.1851)\n",
            "Better model found at epoch 2 with valid_loss value: 1.832078218460083.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2532, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5042, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2269, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2302, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5454, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1600, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2918, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5926, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2608, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2211, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5236, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2175, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2489, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5055, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2165, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2850, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5261, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1261, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2324, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5223, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2212, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2855, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5007, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2549, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2930, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5538, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2118, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2577, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5265, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2308, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2638, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5126, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2539, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2476, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5534, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2481, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2452, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4929, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2551, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3051, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5157, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2364, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2380, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4932, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1934, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2864, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5082, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2502, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2750, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5178, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1926, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2555, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5687, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2073, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2510, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4829, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2985, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1950, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5075, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2245, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3027, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5025, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2597, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2453, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5477, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2448, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2570, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5590, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2211, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2318, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5274, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2666, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1960, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4789, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2348, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2872, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5826, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2168, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2355, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5805, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2873, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2954, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5162, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3271, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2149, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5278, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3019, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2693, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5493, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2059, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2296, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5562, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2168, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2278, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5258, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2364, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2260, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4964, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2502, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2557, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5450, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2399, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2160, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5391, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2051, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2652, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5300, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2700, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3094, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4714, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2181, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2580, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4992, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2114, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2033, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5380, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1803, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2567, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4952, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2572, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.3065, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5321, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2501, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1931, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5388, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2596, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2251, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4742, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2195, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1680, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4989, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2152, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2401, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5184, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2083, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1908, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5528, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2319, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2326, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4955, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1955, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2027)\n",
            "loss1\n",
            "tensor(0.5443)\n",
            "loss2\n",
            "tensor(1.0832)\n",
            "loss0\n",
            "tensor(0.1783)\n",
            "loss1\n",
            "tensor(0.5194)\n",
            "loss2\n",
            "tensor(1.0728)\n",
            "loss0\n",
            "tensor(0.2047)\n",
            "loss1\n",
            "tensor(0.5160)\n",
            "loss2\n",
            "tensor(1.0794)\n",
            "loss0\n",
            "tensor(0.2654)\n",
            "loss1\n",
            "tensor(0.4619)\n",
            "loss2\n",
            "tensor(1.0387)\n",
            "loss0\n",
            "tensor(0.1482)\n",
            "loss1\n",
            "tensor(0.5175)\n",
            "loss2\n",
            "tensor(1.0660)\n",
            "loss0\n",
            "tensor(0.2110)\n",
            "loss1\n",
            "tensor(0.5345)\n",
            "loss2\n",
            "tensor(1.0949)\n",
            "loss0\n",
            "tensor(0.1792)\n",
            "loss1\n",
            "tensor(0.4797)\n",
            "loss2\n",
            "tensor(1.0297)\n",
            "loss0\n",
            "tensor(0.1957)\n",
            "loss1\n",
            "tensor(0.4704)\n",
            "loss2\n",
            "tensor(1.0151)\n",
            "loss0\n",
            "tensor(0.1880)\n",
            "loss1\n",
            "tensor(0.4733)\n",
            "loss2\n",
            "tensor(0.9968)\n",
            "loss0\n",
            "tensor(0.1872)\n",
            "loss1\n",
            "tensor(0.4884)\n",
            "loss2\n",
            "tensor(1.0740)\n",
            "loss0\n",
            "tensor(0.1944)\n",
            "loss1\n",
            "tensor(0.5041)\n",
            "loss2\n",
            "tensor(1.0035)\n",
            "loss0\n",
            "tensor(0.1645)\n",
            "loss1\n",
            "tensor(0.5211)\n",
            "loss2\n",
            "tensor(1.0704)\n",
            "loss0\n",
            "tensor(0.1925)\n",
            "loss1\n",
            "tensor(0.4865)\n",
            "loss2\n",
            "tensor(1.0253)\n",
            "loss0\n",
            "tensor(0.1983)\n",
            "loss1\n",
            "tensor(0.4788)\n",
            "loss2\n",
            "tensor(1.0753)\n",
            "loss0\n",
            "tensor(0.1874)\n",
            "loss1\n",
            "tensor(0.4993)\n",
            "loss2\n",
            "tensor(0.9854)\n",
            "loss0\n",
            "tensor(0.1740)\n",
            "loss1\n",
            "tensor(0.4865)\n",
            "loss2\n",
            "tensor(1.0672)\n",
            "loss0\n",
            "tensor(0.2224)\n",
            "loss1\n",
            "tensor(0.4744)\n",
            "loss2\n",
            "tensor(1.0165)\n",
            "loss0\n",
            "tensor(0.1589)\n",
            "loss1\n",
            "tensor(0.5331)\n",
            "loss2\n",
            "tensor(1.1400)\n",
            "loss0\n",
            "tensor(0.1803)\n",
            "loss1\n",
            "tensor(0.5202)\n",
            "loss2\n",
            "tensor(1.0637)\n",
            "loss0\n",
            "tensor(0.1439)\n",
            "loss1\n",
            "tensor(0.6038)\n",
            "loss2\n",
            "tensor(1.0998)\n",
            "Better model found at epoch 3 with valid_loss value: 1.7447680234909058.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2983, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5529, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2099, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2096, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5213, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1336, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1957, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5932, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2202, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2240, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5095, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1649, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2512, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5201, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1606, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2285, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4953, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1384, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2130, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5048, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1869, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2231, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5296, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1822, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2371, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4883, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.3255, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2062, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5098, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2350, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1821, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4508, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1489, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2456, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5010, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2059, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2131, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5296, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1931, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2136, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4983, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2158, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1888, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4909, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1986, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1930, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5194, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2408, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2175, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5060, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1409, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2730, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5078, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2195, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2183, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4845, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1292, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2819, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5432, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2380, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1944, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5461, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2789, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2049, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4817, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1446, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1890, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5227, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2459, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2085, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5111, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2492, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2332, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5491, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1997, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1581, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4943, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2200, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1924, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4968, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2287, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1764, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4790, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2063, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2339, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4907, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1713, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1977, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5475, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1722, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2342, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5028, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1581, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1983, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5418, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1282, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1532, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5069, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2000, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1901, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4809, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2236, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2094, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5015, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1416, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2066, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5338, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1665, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2151, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5232, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1707, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2006, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4702, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2344, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1658, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5027, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2453, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2139, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4948, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1231, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2615, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4749, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2330, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2142, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5221, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1860, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2591, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5176, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1544, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2250, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4794, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1983, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1987, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4830, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2526, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2319, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5053, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1587, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2162, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4761, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1174, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2101)\n",
            "loss1\n",
            "tensor(0.5112)\n",
            "loss2\n",
            "tensor(0.9946)\n",
            "loss0\n",
            "tensor(0.1712)\n",
            "loss1\n",
            "tensor(0.5142)\n",
            "loss2\n",
            "tensor(1.0587)\n",
            "loss0\n",
            "tensor(0.1690)\n",
            "loss1\n",
            "tensor(0.4980)\n",
            "loss2\n",
            "tensor(0.9737)\n",
            "loss0\n",
            "tensor(0.1702)\n",
            "loss1\n",
            "tensor(0.4681)\n",
            "loss2\n",
            "tensor(1.0343)\n",
            "loss0\n",
            "tensor(0.1805)\n",
            "loss1\n",
            "tensor(0.4879)\n",
            "loss2\n",
            "tensor(1.0259)\n",
            "loss0\n",
            "tensor(0.1469)\n",
            "loss1\n",
            "tensor(0.5102)\n",
            "loss2\n",
            "tensor(0.9700)\n",
            "loss0\n",
            "tensor(0.1563)\n",
            "loss1\n",
            "tensor(0.5308)\n",
            "loss2\n",
            "tensor(1.0162)\n",
            "loss0\n",
            "tensor(0.1595)\n",
            "loss1\n",
            "tensor(0.5436)\n",
            "loss2\n",
            "tensor(1.0626)\n",
            "loss0\n",
            "tensor(0.1398)\n",
            "loss1\n",
            "tensor(0.4568)\n",
            "loss2\n",
            "tensor(1.0193)\n",
            "loss0\n",
            "tensor(0.1736)\n",
            "loss1\n",
            "tensor(0.4784)\n",
            "loss2\n",
            "tensor(1.0489)\n",
            "loss0\n",
            "tensor(0.1718)\n",
            "loss1\n",
            "tensor(0.4814)\n",
            "loss2\n",
            "tensor(0.9584)\n",
            "loss0\n",
            "tensor(0.1910)\n",
            "loss1\n",
            "tensor(0.4734)\n",
            "loss2\n",
            "tensor(0.9634)\n",
            "loss0\n",
            "tensor(0.1363)\n",
            "loss1\n",
            "tensor(0.5223)\n",
            "loss2\n",
            "tensor(1.0465)\n",
            "loss0\n",
            "tensor(0.1791)\n",
            "loss1\n",
            "tensor(0.4859)\n",
            "loss2\n",
            "tensor(0.9924)\n",
            "loss0\n",
            "tensor(0.1769)\n",
            "loss1\n",
            "tensor(0.4805)\n",
            "loss2\n",
            "tensor(1.0695)\n",
            "loss0\n",
            "tensor(0.1625)\n",
            "loss1\n",
            "tensor(0.4989)\n",
            "loss2\n",
            "tensor(1.0202)\n",
            "loss0\n",
            "tensor(0.1760)\n",
            "loss1\n",
            "tensor(0.4972)\n",
            "loss2\n",
            "tensor(0.9425)\n",
            "loss0\n",
            "tensor(0.1529)\n",
            "loss1\n",
            "tensor(0.4594)\n",
            "loss2\n",
            "tensor(1.0338)\n",
            "loss0\n",
            "tensor(0.1813)\n",
            "loss1\n",
            "tensor(0.4685)\n",
            "loss2\n",
            "tensor(1.0256)\n",
            "loss0\n",
            "tensor(0.0969)\n",
            "loss1\n",
            "tensor(0.5013)\n",
            "loss2\n",
            "tensor(1.2228)\n",
            "Better model found at epoch 4 with valid_loss value: 1.6759350299835205.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2279, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5269, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2244, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1938, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5052, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2238, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1983, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4872, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1442, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2133, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5095, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1167, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2030, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5075, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1254, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1495, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5307, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1679, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1781, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5545, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1525, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1786, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5076, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1253, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1641, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5061, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1389, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2298, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5376, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1758, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1951, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5140, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2405, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2550, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4656, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0717, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1876, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4848, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1746, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1747, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4880, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2018, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1950, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5304, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2140, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2061, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5402, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1683, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1921, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4856, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2064, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2284, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4875, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1325, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1744, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4837, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1286, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1595, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5042, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1190, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2283, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4883, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2346, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1780, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5001, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1582, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1865, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5184, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1252, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1835, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4910, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1945, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2191, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5109, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1363, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1570, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5097, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1281, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2001, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5208, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1542, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1728, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5013, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1327, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2369, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4822, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1612, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2170, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4791, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1585, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2166, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4859, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2448, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1910, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5000, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1534, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1409, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4663, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1396, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2088, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4417, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1780, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2371, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5403, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1587, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1548, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4855, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1063, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2438, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5284, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1741, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2018, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4816, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1486, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1766, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4676, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0907, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1555, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4598, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1498, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1858, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4884, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2118, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4642, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2144, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1847, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4652, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1478, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1845, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5228, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1840, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2047, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5581, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0922, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1961, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5102, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2167, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1940, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5202, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1186, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1635)\n",
            "loss1\n",
            "tensor(0.4841)\n",
            "loss2\n",
            "tensor(0.9877)\n",
            "loss0\n",
            "tensor(0.1806)\n",
            "loss1\n",
            "tensor(0.4911)\n",
            "loss2\n",
            "tensor(0.9428)\n",
            "loss0\n",
            "tensor(0.1750)\n",
            "loss1\n",
            "tensor(0.5246)\n",
            "loss2\n",
            "tensor(1.0305)\n",
            "loss0\n",
            "tensor(0.1678)\n",
            "loss1\n",
            "tensor(0.5122)\n",
            "loss2\n",
            "tensor(0.9916)\n",
            "loss0\n",
            "tensor(0.1494)\n",
            "loss1\n",
            "tensor(0.4944)\n",
            "loss2\n",
            "tensor(1.0225)\n",
            "loss0\n",
            "tensor(0.1691)\n",
            "loss1\n",
            "tensor(0.5119)\n",
            "loss2\n",
            "tensor(0.9651)\n",
            "loss0\n",
            "tensor(0.1836)\n",
            "loss1\n",
            "tensor(0.4507)\n",
            "loss2\n",
            "tensor(0.9529)\n",
            "loss0\n",
            "tensor(0.1842)\n",
            "loss1\n",
            "tensor(0.5087)\n",
            "loss2\n",
            "tensor(0.9983)\n",
            "loss0\n",
            "tensor(0.1795)\n",
            "loss1\n",
            "tensor(0.4909)\n",
            "loss2\n",
            "tensor(0.9791)\n",
            "loss0\n",
            "tensor(0.1562)\n",
            "loss1\n",
            "tensor(0.5058)\n",
            "loss2\n",
            "tensor(0.9657)\n",
            "loss0\n",
            "tensor(0.1772)\n",
            "loss1\n",
            "tensor(0.4927)\n",
            "loss2\n",
            "tensor(1.0349)\n",
            "loss0\n",
            "tensor(0.1574)\n",
            "loss1\n",
            "tensor(0.5238)\n",
            "loss2\n",
            "tensor(0.9703)\n",
            "loss0\n",
            "tensor(0.1869)\n",
            "loss1\n",
            "tensor(0.4961)\n",
            "loss2\n",
            "tensor(0.9528)\n",
            "loss0\n",
            "tensor(0.1355)\n",
            "loss1\n",
            "tensor(0.4833)\n",
            "loss2\n",
            "tensor(0.9273)\n",
            "loss0\n",
            "tensor(0.1599)\n",
            "loss1\n",
            "tensor(0.4410)\n",
            "loss2\n",
            "tensor(0.9759)\n",
            "loss0\n",
            "tensor(0.1767)\n",
            "loss1\n",
            "tensor(0.4577)\n",
            "loss2\n",
            "tensor(1.0366)\n",
            "loss0\n",
            "tensor(0.1504)\n",
            "loss1\n",
            "tensor(0.4369)\n",
            "loss2\n",
            "tensor(0.9427)\n",
            "loss0\n",
            "tensor(0.1363)\n",
            "loss1\n",
            "tensor(0.4910)\n",
            "loss2\n",
            "tensor(1.0231)\n",
            "loss0\n",
            "tensor(0.1694)\n",
            "loss1\n",
            "tensor(0.4978)\n",
            "loss2\n",
            "tensor(1.0022)\n",
            "loss0\n",
            "tensor(0.2168)\n",
            "loss1\n",
            "tensor(0.4612)\n",
            "loss2\n",
            "tensor(1.0362)\n",
            "Better model found at epoch 5 with valid_loss value: 1.6401439905166626.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2320, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4893, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1398, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1638, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4813, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1483, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2029, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4831, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2123, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1811, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4793, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1411, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1812, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4747, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1695, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1861, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5054, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1049, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1421, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4655, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1839, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2328, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5040, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1207, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2047, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4942, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1900, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2337, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4848, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2053, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1991, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4856, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1724, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1826, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4708, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1652, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1639, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5075, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1421, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1620, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4723, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1403, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1694, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4678, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1318, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2259, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4467, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1241, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1523, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5149, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0354, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1737, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5106, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1132, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2082, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5151, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1197, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1888, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5042, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1650, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1457, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4632, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1139, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1268, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4848, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1098, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1550, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4830, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1354, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1812, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4823, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1048, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1782, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4854, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1383, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1247, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4717, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1361, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1679, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4934, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1844, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1856, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4903, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0234, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2267, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4484, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1800, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4826, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1275, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1566, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4955, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1287, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1567, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4917, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0962, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2085, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4692, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0866, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1880, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5069, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1425, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1694, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5555, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1228, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1893, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5187, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1370, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4812, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0838, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1293, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5323, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1322, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5145, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1591, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1762, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5201, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1398, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1498, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5101, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1634, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1507, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4908, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1858, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4795, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1936, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2145, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5085, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1126, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1753, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4700, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1587, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1859, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5329, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1104, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1294, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4494, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0504, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1615)\n",
            "loss1\n",
            "tensor(0.4902)\n",
            "loss2\n",
            "tensor(0.9984)\n",
            "loss0\n",
            "tensor(0.1328)\n",
            "loss1\n",
            "tensor(0.4976)\n",
            "loss2\n",
            "tensor(0.9683)\n",
            "loss0\n",
            "tensor(0.1615)\n",
            "loss1\n",
            "tensor(0.4643)\n",
            "loss2\n",
            "tensor(0.9700)\n",
            "loss0\n",
            "tensor(0.1606)\n",
            "loss1\n",
            "tensor(0.5075)\n",
            "loss2\n",
            "tensor(0.9593)\n",
            "loss0\n",
            "tensor(0.1354)\n",
            "loss1\n",
            "tensor(0.4354)\n",
            "loss2\n",
            "tensor(0.9607)\n",
            "loss0\n",
            "tensor(0.1336)\n",
            "loss1\n",
            "tensor(0.4574)\n",
            "loss2\n",
            "tensor(0.9926)\n",
            "loss0\n",
            "tensor(0.1435)\n",
            "loss1\n",
            "tensor(0.4853)\n",
            "loss2\n",
            "tensor(0.9501)\n",
            "loss0\n",
            "tensor(0.1508)\n",
            "loss1\n",
            "tensor(0.4714)\n",
            "loss2\n",
            "tensor(0.9715)\n",
            "loss0\n",
            "tensor(0.1440)\n",
            "loss1\n",
            "tensor(0.4803)\n",
            "loss2\n",
            "tensor(0.9665)\n",
            "loss0\n",
            "tensor(0.1612)\n",
            "loss1\n",
            "tensor(0.5079)\n",
            "loss2\n",
            "tensor(0.9669)\n",
            "loss0\n",
            "tensor(0.1603)\n",
            "loss1\n",
            "tensor(0.4596)\n",
            "loss2\n",
            "tensor(0.9749)\n",
            "loss0\n",
            "tensor(0.1455)\n",
            "loss1\n",
            "tensor(0.4783)\n",
            "loss2\n",
            "tensor(0.9141)\n",
            "loss0\n",
            "tensor(0.1615)\n",
            "loss1\n",
            "tensor(0.4650)\n",
            "loss2\n",
            "tensor(0.9088)\n",
            "loss0\n",
            "tensor(0.1669)\n",
            "loss1\n",
            "tensor(0.4580)\n",
            "loss2\n",
            "tensor(0.9695)\n",
            "loss0\n",
            "tensor(0.1387)\n",
            "loss1\n",
            "tensor(0.4989)\n",
            "loss2\n",
            "tensor(0.9162)\n",
            "loss0\n",
            "tensor(0.1845)\n",
            "loss1\n",
            "tensor(0.4969)\n",
            "loss2\n",
            "tensor(0.9848)\n",
            "loss0\n",
            "tensor(0.1763)\n",
            "loss1\n",
            "tensor(0.5167)\n",
            "loss2\n",
            "tensor(0.9883)\n",
            "loss0\n",
            "tensor(0.1679)\n",
            "loss1\n",
            "tensor(0.4512)\n",
            "loss2\n",
            "tensor(0.9654)\n",
            "loss0\n",
            "tensor(0.1554)\n",
            "loss1\n",
            "tensor(0.4807)\n",
            "loss2\n",
            "tensor(1.0098)\n",
            "loss0\n",
            "tensor(0.1867)\n",
            "loss1\n",
            "tensor(0.4312)\n",
            "loss2\n",
            "tensor(1.1019)\n",
            "Better model found at epoch 6 with valid_loss value: 1.5995968580245972.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.2084, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4928, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1387, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1283, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4936, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1264, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1727, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4852, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1101, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1683, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5162, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0820, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1507, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5106, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1179, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1797, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4595, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1954, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1929, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4965, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1928, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5043, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1410, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1614, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5340, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1717, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1995, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4745, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1752, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1361, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4883, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1308, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1813, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5116, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1948, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1635, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4517, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0567, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1409, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4990, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1242, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1977, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4640, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0683, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1503, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4972, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1090, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1681, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4600, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1499, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1761, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5222, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1231, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1785, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4573, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1404, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4697, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0937, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1548, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4920, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1180, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1354, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5140, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0626, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1476, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4926, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1450, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1741, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5220, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1582, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2012, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5095, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1193, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1620, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5365, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1441, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1815, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5072, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1264, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1630, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5198, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1444, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1319, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4757, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0864, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1485, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4796, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1350, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1517, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4622, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0807, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1577, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5042, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1370, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1557, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5041, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1357, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1349, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4642, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1094, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1797, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4921, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1132, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1598, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5185, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0540, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1563, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4998, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0881, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1835, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4762, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1463, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1941, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5081, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0897, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1295, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4839, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1178, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1774, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4563, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1589, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1673, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4736, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0990, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1522, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5190, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1147, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1739, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4708, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1308, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1744, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4507, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1828, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1829, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5015, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0447, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1270, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4003, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2180, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1908)\n",
            "loss1\n",
            "tensor(0.5091)\n",
            "loss2\n",
            "tensor(1.0013)\n",
            "loss0\n",
            "tensor(0.1534)\n",
            "loss1\n",
            "tensor(0.4793)\n",
            "loss2\n",
            "tensor(1.0002)\n",
            "loss0\n",
            "tensor(0.1489)\n",
            "loss1\n",
            "tensor(0.4658)\n",
            "loss2\n",
            "tensor(0.9849)\n",
            "loss0\n",
            "tensor(0.1209)\n",
            "loss1\n",
            "tensor(0.4887)\n",
            "loss2\n",
            "tensor(0.9416)\n",
            "loss0\n",
            "tensor(0.1384)\n",
            "loss1\n",
            "tensor(0.4520)\n",
            "loss2\n",
            "tensor(0.9144)\n",
            "loss0\n",
            "tensor(0.1482)\n",
            "loss1\n",
            "tensor(0.5159)\n",
            "loss2\n",
            "tensor(0.9243)\n",
            "loss0\n",
            "tensor(0.1629)\n",
            "loss1\n",
            "tensor(0.4713)\n",
            "loss2\n",
            "tensor(0.9848)\n",
            "loss0\n",
            "tensor(0.1199)\n",
            "loss1\n",
            "tensor(0.4523)\n",
            "loss2\n",
            "tensor(0.9868)\n",
            "loss0\n",
            "tensor(0.1356)\n",
            "loss1\n",
            "tensor(0.4684)\n",
            "loss2\n",
            "tensor(0.9057)\n",
            "loss0\n",
            "tensor(0.1296)\n",
            "loss1\n",
            "tensor(0.4752)\n",
            "loss2\n",
            "tensor(0.9561)\n",
            "loss0\n",
            "tensor(0.1373)\n",
            "loss1\n",
            "tensor(0.4704)\n",
            "loss2\n",
            "tensor(0.9302)\n",
            "loss0\n",
            "tensor(0.1811)\n",
            "loss1\n",
            "tensor(0.4610)\n",
            "loss2\n",
            "tensor(0.9368)\n",
            "loss0\n",
            "tensor(0.1554)\n",
            "loss1\n",
            "tensor(0.4490)\n",
            "loss2\n",
            "tensor(0.8859)\n",
            "loss0\n",
            "tensor(0.1240)\n",
            "loss1\n",
            "tensor(0.5013)\n",
            "loss2\n",
            "tensor(0.9138)\n",
            "loss0\n",
            "tensor(0.1400)\n",
            "loss1\n",
            "tensor(0.4424)\n",
            "loss2\n",
            "tensor(0.9791)\n",
            "loss0\n",
            "tensor(0.1641)\n",
            "loss1\n",
            "tensor(0.4822)\n",
            "loss2\n",
            "tensor(1.0283)\n",
            "loss0\n",
            "tensor(0.1303)\n",
            "loss1\n",
            "tensor(0.5113)\n",
            "loss2\n",
            "tensor(0.9489)\n",
            "loss0\n",
            "tensor(0.1384)\n",
            "loss1\n",
            "tensor(0.4294)\n",
            "loss2\n",
            "tensor(0.9632)\n",
            "loss0\n",
            "tensor(0.1633)\n",
            "loss1\n",
            "tensor(0.4989)\n",
            "loss2\n",
            "tensor(1.0369)\n",
            "loss0\n",
            "tensor(0.2110)\n",
            "loss1\n",
            "tensor(0.4829)\n",
            "loss2\n",
            "tensor(1.1623)\n",
            "Better model found at epoch 7 with valid_loss value: 1.5819365978240967.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1568, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4833, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0588, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1641, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4612, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1421, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1499, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5187, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0646, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1435, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4933, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0136, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1477, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4895, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0444, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1310, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4689, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0482, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1526, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4889, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1606, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4691, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1662, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1876, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4525, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1227, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1777, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4553, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1566, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1553, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5140, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1116, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1308, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5023, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1044, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1492, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4905, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1154, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1499, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4903, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0738, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1456, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4882, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0919, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1607, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5086, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0841, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1456, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4926, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0687, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1827, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4687, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1553, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1747, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4603, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1495, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1575, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4759, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1127, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1374, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5140, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0436, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1679, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5151, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1679, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1830, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4713, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1139, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1732, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4698, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1498, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1339, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4308, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1488, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1282, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5204, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1313, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1284, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4828, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1258, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1629, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4941, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1615, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1638, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5143, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0322, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1595, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5168, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1296, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1673, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4778, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0727, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1753, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4881, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1292, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1933, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4763, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1983, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1446, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5173, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1619, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1271, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4818, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0698, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1379, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4712, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1279, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1440, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5002, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1035, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1404, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4699, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1031, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1528, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4718, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0744, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2264, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5029, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1181, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1319, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4618, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1169, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1791, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5099, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1657, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1382, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4563, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0830, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1142, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4844, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0471, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1758, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4355, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1162, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1308, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4909, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1348, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2832, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5527, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1018, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1482)\n",
            "loss1\n",
            "tensor(0.4964)\n",
            "loss2\n",
            "tensor(1.0169)\n",
            "loss0\n",
            "tensor(0.1273)\n",
            "loss1\n",
            "tensor(0.4876)\n",
            "loss2\n",
            "tensor(0.9928)\n",
            "loss0\n",
            "tensor(0.1251)\n",
            "loss1\n",
            "tensor(0.4607)\n",
            "loss2\n",
            "tensor(0.9744)\n",
            "loss0\n",
            "tensor(0.1416)\n",
            "loss1\n",
            "tensor(0.4831)\n",
            "loss2\n",
            "tensor(1.0013)\n",
            "loss0\n",
            "tensor(0.1664)\n",
            "loss1\n",
            "tensor(0.4887)\n",
            "loss2\n",
            "tensor(0.9697)\n",
            "loss0\n",
            "tensor(0.1687)\n",
            "loss1\n",
            "tensor(0.4526)\n",
            "loss2\n",
            "tensor(0.9768)\n",
            "loss0\n",
            "tensor(0.1379)\n",
            "loss1\n",
            "tensor(0.4612)\n",
            "loss2\n",
            "tensor(0.9722)\n",
            "loss0\n",
            "tensor(0.1692)\n",
            "loss1\n",
            "tensor(0.4454)\n",
            "loss2\n",
            "tensor(0.9164)\n",
            "loss0\n",
            "tensor(0.1352)\n",
            "loss1\n",
            "tensor(0.4622)\n",
            "loss2\n",
            "tensor(0.9799)\n",
            "loss0\n",
            "tensor(0.1644)\n",
            "loss1\n",
            "tensor(0.5036)\n",
            "loss2\n",
            "tensor(0.9722)\n",
            "loss0\n",
            "tensor(0.1259)\n",
            "loss1\n",
            "tensor(0.4682)\n",
            "loss2\n",
            "tensor(0.9569)\n",
            "loss0\n",
            "tensor(0.1250)\n",
            "loss1\n",
            "tensor(0.4834)\n",
            "loss2\n",
            "tensor(0.9707)\n",
            "loss0\n",
            "tensor(0.1406)\n",
            "loss1\n",
            "tensor(0.4929)\n",
            "loss2\n",
            "tensor(1.0002)\n",
            "loss0\n",
            "tensor(0.1679)\n",
            "loss1\n",
            "tensor(0.4781)\n",
            "loss2\n",
            "tensor(0.8987)\n",
            "loss0\n",
            "tensor(0.1401)\n",
            "loss1\n",
            "tensor(0.4877)\n",
            "loss2\n",
            "tensor(0.8688)\n",
            "loss0\n",
            "tensor(0.1350)\n",
            "loss1\n",
            "tensor(0.4443)\n",
            "loss2\n",
            "tensor(0.9564)\n",
            "loss0\n",
            "tensor(0.1382)\n",
            "loss1\n",
            "tensor(0.4737)\n",
            "loss2\n",
            "tensor(0.9458)\n",
            "loss0\n",
            "tensor(0.1596)\n",
            "loss1\n",
            "tensor(0.4815)\n",
            "loss2\n",
            "tensor(1.0071)\n",
            "loss0\n",
            "tensor(0.1349)\n",
            "loss1\n",
            "tensor(0.4616)\n",
            "loss2\n",
            "tensor(0.9013)\n",
            "loss0\n",
            "tensor(0.1437)\n",
            "loss1\n",
            "tensor(0.5897)\n",
            "loss2\n",
            "tensor(0.7646)\n",
            "Better model found at epoch 8 with valid_loss value: 1.5807337760925293.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1360, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5111, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1213, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1471, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4715, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0724, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1533, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4731, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1264, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2185, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4902, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1423, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1404, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5121, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0162, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1249, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5011, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1299, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1194, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5230, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0537, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1370, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4802, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1152, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1450, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5049, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1065, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1443, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4835, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1224, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1787, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4724, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0904, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1903, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4683, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0443, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1673, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5088, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0472, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1840, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4806, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0875, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1499, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5104, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1690, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1574, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5112, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0636, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1718, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5133, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1316, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1569, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4839, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0431, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1575, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5308, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2014, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1561, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4981, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1824, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1691, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4870, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1589, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1367, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4657, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1754, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4787, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1942, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4951, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1561, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2244, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4313, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1258, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1397, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4426, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1011, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1531, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5011, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0480, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1884, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4812, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1469, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1506, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4844, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0667, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1469, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4735, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1094, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1300, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5096, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0876, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1603, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5100, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0996, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1743, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4674, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1280, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1477, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4981, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0377, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1885, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4566, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0466, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1821, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4819, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1359, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1642, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4597, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1421, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4889, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1060, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1174, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4894, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.2157, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1589, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4438, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0676, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1545, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4738, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0938, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2184, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4797, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0922, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1795, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4937, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1921, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1657, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.5001, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1699, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4479, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1898, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.1679, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4814, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1098, grad_fn=<NllLossBackward0>)\n",
            "loss0\n",
            "tensor(0.2239, grad_fn=<AddBackward0>)\n",
            "loss1\n",
            "tensor(0.4949, grad_fn=<NllLossBackward0>)\n",
            "loss2\n",
            "tensor(1.1003, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0\n",
            "tensor(0.1460)\n",
            "loss1\n",
            "tensor(0.4746)\n",
            "loss2\n",
            "tensor(0.8680)\n",
            "loss0\n",
            "tensor(0.1597)\n",
            "loss1\n",
            "tensor(0.5025)\n",
            "loss2\n",
            "tensor(0.9947)\n",
            "loss0\n",
            "tensor(0.1496)\n",
            "loss1\n",
            "tensor(0.4866)\n",
            "loss2\n",
            "tensor(0.9274)\n",
            "loss0\n",
            "tensor(0.1400)\n",
            "loss1\n",
            "tensor(0.4521)\n",
            "loss2\n",
            "tensor(0.9559)\n",
            "loss0\n",
            "tensor(0.1583)\n",
            "loss1\n",
            "tensor(0.4951)\n",
            "loss2\n",
            "tensor(0.9496)\n",
            "loss0\n",
            "tensor(0.1369)\n",
            "loss1\n",
            "tensor(0.4866)\n",
            "loss2\n",
            "tensor(0.9192)\n",
            "loss0\n",
            "tensor(0.1487)\n",
            "loss1\n",
            "tensor(0.4583)\n",
            "loss2\n",
            "tensor(0.9547)\n",
            "loss0\n",
            "tensor(0.1400)\n",
            "loss1\n",
            "tensor(0.4563)\n",
            "loss2\n",
            "tensor(0.9667)\n",
            "loss0\n",
            "tensor(0.1361)\n",
            "loss1\n",
            "tensor(0.4988)\n",
            "loss2\n",
            "tensor(1.0076)\n",
            "loss0\n",
            "tensor(0.1436)\n",
            "loss1\n",
            "tensor(0.4834)\n",
            "loss2\n",
            "tensor(0.9285)\n",
            "loss0\n",
            "tensor(0.1834)\n",
            "loss1\n",
            "tensor(0.4414)\n",
            "loss2\n",
            "tensor(1.0308)\n",
            "loss0\n",
            "tensor(0.1260)\n",
            "loss1\n",
            "tensor(0.5136)\n",
            "loss2\n",
            "tensor(0.9457)\n",
            "loss0\n",
            "tensor(0.1038)\n",
            "loss1\n",
            "tensor(0.4802)\n",
            "loss2\n",
            "tensor(0.9601)\n",
            "loss0\n",
            "tensor(0.1348)\n",
            "loss1\n",
            "tensor(0.4703)\n",
            "loss2\n",
            "tensor(0.9135)\n",
            "loss0\n",
            "tensor(0.1468)\n",
            "loss1\n",
            "tensor(0.4582)\n",
            "loss2\n",
            "tensor(0.9823)\n",
            "loss0\n",
            "tensor(0.1584)\n",
            "loss1\n",
            "tensor(0.4682)\n",
            "loss2\n",
            "tensor(0.9141)\n",
            "loss0\n",
            "tensor(0.1224)\n",
            "loss1\n",
            "tensor(0.4416)\n",
            "loss2\n",
            "tensor(1.0010)\n",
            "loss0\n",
            "tensor(0.1456)\n",
            "loss1\n",
            "tensor(0.4646)\n",
            "loss2\n",
            "tensor(0.9962)\n",
            "loss0\n",
            "tensor(0.1428)\n",
            "loss1\n",
            "tensor(0.4842)\n",
            "loss2\n",
            "tensor(0.8641)\n",
            "loss0\n",
            "tensor(0.2195)\n",
            "loss1\n",
            "tensor(0.4149)\n",
            "loss2\n",
            "tensor(1.1582)\n",
            "Better model found at epoch 9 with valid_loss value: 1.5706210136413574.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = learn.model.cpu()\n",
        "torch.save(trained_model.state_dict(),\"model_params_resnet34\")"
      ],
      "metadata": {
        "id": "eTNLXmosBKog"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inference():\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.tfms = get_transforms()[1]\n",
        "        self.norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #imagenet stats\n",
        "        self.gender = {0:\"Male\",1:\"Female\"}\n",
        "        self.ethnicity = {0:\"White\",1:\"Black\",2:\"Asian\",3:\"Indian\",4:\"Others\"}\n",
        "\n",
        "    def predict(self,x):\n",
        "        #x is a PIL Image\n",
        "        x = Image(pil2tensor(x, dtype=np.float32).div_(255))\n",
        "        x = x.apply_tfms(self.tfms, size = 64)\n",
        "        x = self.norm(x.data)\n",
        "        preds = self.model(x.unsqueeze(0))\n",
        "        age = int(torch.exp(preds[0]*4.75).item())\n",
        "        gender = self.gender[torch.softmax(preds[1],1).argmax().item()]\n",
        "        ethnicity = self.ethnicity[torch.softmax(preds[2],1).argmax().item()]\n",
        "        return age, gender, ethnicity"
      ],
      "metadata": {
        "id": "TkHZT27sEVF8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Inference = Inference(trained_model)"
      ],
      "metadata": {
        "id": "6Cb_dJYXVPq-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = PIL.Image.open(df_validation.name.iloc[130])\n",
        "img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "L11f_5feVWRK",
        "outputId": "35954de3-2e3b-404c-b7c0-3e9ad35d0ade"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x200 at 0x7FED77923350>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Inference.predict(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxW5BuOdWYNb",
        "outputId": "2948c791-a2d3-43c6-de78-8437f6c60960"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48, 'Female', 'White')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}